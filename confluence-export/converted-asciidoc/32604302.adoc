1.  link:index.html[Apache Solr Reference Guide]
2.  link:Apache-Solr-Reference-Guide.html[Apache Solr Reference Guide]
3.  link:SolrCloud.html[SolrCloud]
4.  link:How-SolrCloud-Works.html[How SolrCloud Works]

32604302
--------

[[NRT,Replication,andDisasterRecoverywithSolrCloud-SolrCloudandReplication]]
SolrCloud and Replication
~~~~~~~~~~~~~~~~~~~~~~~~~

Replication ensures redundancy for your data, and enables you to send an update request to any node in the shard. If that node is a replica, it will forward the request to the leader, which then forwards it to all existing replicas, using versioning to make sure every replica has the most up-to-date version. This architecture enables you to be certain that your data can be recovered in the event of a disaster, even if you are using Near Real Time searching.

[[NRT,Replication,andDisasterRecoverywithSolrCloud-NearRealTimeSearching]]
Near Real Time Searching
~~~~~~~~~~~~~~~~~~~~~~~~

If you want to use the http://wiki.apache.org/solr/NearRealtimeSearch[NearRealtimeSearch] support, enable auto soft commits in your `solrconfig.xml` file before storing it into Zookeeper. Otherwise you can send explicit soft commits to the cluster as you need.

SolrCloud doesn't work very well with separated data clusters connected by an expensive pipe. The root problem is that SolrCloud's architecture sends documents to all the nodes in the cluster (on a per-shard basis), and that architecture is really dictated by the NRT functionality.

Imagine that you have a set of servers in China and one in the US that are aware of each other. Assuming 5 replicas, a single update to a shard may make multiple trips over the expensive pipe before it's all done, probably slowing indexing speed unacceptably.

So the SolrCloud recommendation for this situation is to maintain these clusters separately; nodes in China don't even know that nodes exist in the US and vice-versa. When indexing, you send the update request to one node in the US and one in China and all the node-routing after that is local to the separate clusters. Requests can go to any node in either country and maintain a consistent view of the data.

However, if your US cluster goes down, you have to re-synchronize the down cluster with up-to-date information from China. The process requires you to replicate the index from China to the repaired US installation and then get everything back up and working.

[[NRT,Replication,andDisasterRecoverywithSolrCloud-DisasterRecoveryforanNRTsystem]]
Disaster Recovery for an NRT system
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use of Near Real Time (NRT) searching affects the way that systems using SolrCloud behave during disaster recovery.

The procedure outlined below assumes that you are maintaining separate clusters, as described above. Consider, for example, an event in which the US cluster goes down (say, because of a hurricane), but the China cluster is intact. Disaster recovery consists of creating the new system and letting the intact cluster create a replicate for each shard on it, then promoting those replicas to be leaders of the newly created US cluster.

Here are the steps to take:

1.  Take the downed system offline to all end users.
2.  Take the indexing process offline.
3.  Repair the system.
4.  Bring up one machine per shard in the repaired system as part of the ZooKeeper cluster on the good system, and wait for replication to happen, creating a replica on that machine. (SoftCommits will not be repeated, but data will be pulled from the transaction logs if necessary.) Note:
+
SolrCloud will automatically use old-style replication for the bulk load. By temporarily having only one replica, you'll minimize data transfer across a slow connection.
5.  Bring the machines of the repaired cluster down, and reconfigure them to be a separate Zookeeper cluster again, optionally adding more replicas for each shard.
6.  Make the repaired system visible to end users again.
7.  Start the indexing program again, delivering updates to both systems.

