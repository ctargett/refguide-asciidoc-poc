= Streaming Expressions
:page-shortname: streaming-expressions
:page-permalink: streaming-expressions.html
:page-children: graph-traversal

Streaming Expressions provide a simple yet powerful stream processing language for SolrCloud. They are a suite of functions that can be combined to perform many different parallel computing tasks. These functions are the basis for the <<parallel-sql-interface.adoc#,Parallel SQL Interface>>.

There are several available functions, including those that implement:

* continuous push streaming
* continuous pull streaming
* request/response streaming
* MapReduce shuffling aggregation
* pushdown faceted aggregation
* parallel relational algebra (distributed joins, intersections, unions, complements)
* publish/subscribe messaging
* distributed graph traversal (Solr 6.1)

Streams from outside systems can be joined with streams originating from Solr and users can add their own stream functions by following Solr's {solr-javadocs}/solr-solrj/org/apache/solr/client/solrj/io/stream/package-summary.html[Java streaming API].

[IMPORTANT]
====

Both streaming expressions and the streaming API are considered experimental, and the APIs are subject to change.

====

[[StreamingExpressions-StreamLanguageBasics]]
== Stream Language Basics

Streaming Expressions are comprised of streaming functions which work with a Solr collection. They emit a stream of tuples (key/value Maps).

Many of the provided streaming functions are designed to work with entire result sets rather then the top N results like normal search. This is supported by the <<exporting-result-sets.adoc#,/export handler>>.

Some streaming functions act as stream sources to originate the stream flow. Other streaming functions act as stream decorators to wrap other stream functions and perform operations on the stream of tuples. Many streams functions can be parallelized across a worker collection. This can be particularly powerful for relational algebra functions.

[[StreamingExpressions-StreamingRequestsandResponses]]
=== Streaming Requests and Responses

Solr has a `/stream` request handler that takes streaming expression requests and returns the tuples as a JSON stream. This request handler is implicitly defined, meaning there is nothing that has to be defined in `solrconfig.xml`.

The `/stream` request handler takes one parameter, `expr`, which is used to specify the streaming expression. For example, this curl command encodes and POSTs a simple `search()` expression to the `/stream` handler:

[source,bash]
----
curl --data-urlencode 'expr=search(enron_emails, 
                                   q="from:1800flowers*", 
                                   fl="from, to", 
                                   sort="from asc", 
                                   qt="/export")' http://localhost:8983/solr/enron_emails/stream
----

Details of the parameters for each function are included below.

For the above example the `/stream` handler responded with the following JSON response:

[source,java]
----
{"result-set":{"docs":[
   {"from":"1800flowers.133139412@s2u2.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers.93690065@s2u2.com","to":"jtholt@ect.enron.com"},
   {"from":"1800flowers.96749439@s2u2.com","to":"alewis@enron.com"},
   {"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@1800flowers.flonetwork.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"lcampbel@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
   {"from":"1800flowers@shop2u.com","to":"ebass@enron.com"},
   {"EOF":true,"RESPONSE_TIME":33}]}
}
----

Note the last tuple in the above example stream is `{"EOF":true,"RESPONSE_TIME":33}`. The `EOF` indicates the end of the stream. To process the JSON response, you'll need to use a streaming JSON implementation because streaming expressions are designed to return the entire result set which may have millions of records. In your JSON client you'll need to iterate each doc (tuple) and check for the EOF tuple to determine the end of stream.

The {solr-javadocs}/solr-solrj/org/apache/solr/client/solrj/io/package-summary.html[`org.apache.solr.client.solrj.io`] package provides Java classes that compile streaming expressions into streaming API objects. These classes can be used to execute streaming expressions from inside a Java application. For example:

[source,java]
----
StreamFactory streamFactory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress())
    .withStreamFunction("search", CloudSolrStream.class)
    .withStreamFunction("unique", UniqueStream.class)
    .withStreamFunction("top", RankStream.class)
    .withStreamFunction("group", ReducerStream.class)
    .withStreamFunction("parallel", ParallelStream.class);
 
ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1, group(search(collection1, q=\"*:*\", fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc,a_f asc\", partitionKeys=\"a_s\"), by=\"a_s asc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_s asc\")");
----

[[StreamingExpressions-DataRequirements]]
=== Data Requirements

Because streaming expressions relies on the `/export` handler, many of the field and field type requirements to use `/export` are also requirements for `/stream`, particularly for `sort` and `fl` parameters. Please see the section <<exporting-result-sets.adoc#,Exporting Result Sets>> for details.

[[StreamingExpressions-StreamSources]]
== Stream Sources

Stream sources originate streams. There are several stream sources available: **search**, **jdbc**, **facet**, **gatherNodes**, **random**, **stats**, **topic**, and **shortestPath**.

[[StreamingExpressions-search]]
=== search

The `search` function searches a SolrCloud collection and emits a stream of tuples that match the query. This is very similar to a standard Solr query, and uses many of the same parameters.

This expression allows you to specify a request hander using the `qt` parameter. By default, the `/select` handler is used. The `/select` handler can be used for simple rapid prototyping of expressions. For production, however, you will most likely want to use the `/export` handler which is designed to `sort` and `export` entire result sets. The `/export` handler is not used by default because it has stricter requirements then the `/select` handler so it's not as easy to get started working with. To read more about the `/export` handler requirements review the section <<exporting-result-sets.adoc#,Exporting Result Sets>>.

[[StreamingExpressions-Parameters]]
==== Parameters

* `collection`: (Mandatory) the collection being searched.
* `q`: (Mandatory) The query to perform on the Solr index.
* `fl`: (Mandatory) The list of fields to return.
* `sort`: (Mandatory) The sort criteria.
* `zkHost`: Only needs to be defined if the collection being searched is found in a different zkHost than the local stream handler.
* `qt`: Specifies the query type, or request handler, to use. Set this to `/export` to work with large result sets. The default is `/select`.
* `rows`: (Mandatory with the `/select` handler) The rows parameter specifies how many rows to return. This parameter is only needed with the `/select` handler (which is the default) since the `/export` handler always returns all rows.

[[StreamingExpressions-Syntax]]
==== Syntax

[source,java]
----
expr=search(collection1, 
       zkHost="localhost:9983",
       qt="/export", 
       q="*:*", 
       fl="id,a_s,a_i,a_f", 
       sort="a_f asc, a_i asc") 
----

[[StreamingExpressions-jdbc]]
=== jdbc

The `jdbc` function searches a JDBC datasource and emits a stream of tuples representing the JDBC result set. Each row in the result set is translated into a tuple and each tuple contains all the cell values for that row.

[[StreamingExpressions-Parameters.1]]
==== Parameters

* `connection`: (Mandatory) JDBC formatted connection string to whatever driver you are using.
* `sql`: (Mandatory) query to pass off to the JDBC endpoint
* `sort`: (Mandatory) The sort criteria indicating how the data coming out of the JDBC stream is sorted
* `driver`: The name of the JDBC driver used for the connection. If provided then the driver class will attempt to be loaded into the JVM. If not provided then it is assumed that the driver is already loaded into the JVM. Some drivers require explicit loading so this option is provided.
* `[driverProperty]`: One or more properties to pass to the JDBC driver during connection. The format is `propertyName="propertyValue"`. You can provide as many of these properties as you'd like and they will all be passed to the connection.

[[StreamingExpressions-ConnectionsandDrivers]]
==== Connections and Drivers

Because some JDBC drivers require explicit loading the `driver` parameter can be used to provide the driver class name. If provided, then during stream construction the driver will be loaded. If the driver cannot be loaded because the class is not found on the classpath, then stream construction will fail.

When the JDBC stream is opened it will validate that a driver can be found for the provided connection string. If a driver cannot be found (because it hasn't been loaded) then the open will fail.

[[StreamingExpressions-Datatypes]]
==== Datatypes

Due to the inherent differences in datatypes across JDBC sources the following datatypes are supported. The table indicates what Java type will be used for a given JDBC type. Types marked as requiring conversion will go through a conversion for each value of that type. For performance reasons the cell data types are only considered when the stream is opened as this is when the converters are created.

[width="100%",cols="34%,33%,33%",options="header",]
|===
|JDBC Type |Java Type |Requires Conversion
|String |String |No
|Short |Long |Yes
|Integer |Long |Yes
|Long |Long |No
|Float |Double |Yes
|Double |Double |No
|Boolean |Boolean |No
|===

[[StreamingExpressions-Syntax.1]]
==== Syntax

A basic `jdbc` expression:

[source,java]
----
jdbc(
    connection="jdbc:hsqldb:mem:.", 
    sql="select NAME, ADDRESS, EMAIL, AGE from PEOPLE where AGE > 25 order by AGE, NAME DESC", 
    sort="AGE asc, NAME desc",
    driver="org.hsqldb.jdbcDriver"
)
----

A `jdbc` expression that passes a property to the driver:

[source,java]
----
// get_column_name is a property to pass to the hsqldb driver
jdbc(
    connection="jdbc:hsqldb:mem:.", 
    sql="select NAME as FIRST_NAME, ADDRESS, EMAIL, AGE from PEOPLE where AGE > 25 order by AGE, NAME DESC", 
    sort="AGE asc, NAME desc",
    driver="org.hsqldb.jdbcDriver",
    get_column_name="false"
)
----

[[StreamingExpressions-facet]]
=== facet

The `facet` function provides aggregations that are rolled up over buckets. Under the covers the facet function pushes down the aggregation into the search engine using Solr's JSON Facet API. This provides sub-second performance for many use cases. The facet function is appropriate for use with a low to moderate number of distinct values in the bucket fields. To support high cardinality aggregations see the rollup function.

[[StreamingExpressions-Parameters.2]]
==== Parameters

* `collection`: (Mandatory) Collection the facets will be aggregated from.
* `q`: (Mandatory) The query to build the aggregations from.
* `buckets`: (Mandatory) Comma separated list of fields to rollup over. The comma separated list represents the dimensions in a multi-dimensional rollup.
* `bucketSorts`: Comma separated list of sorts to apply to each dimension in the buckets parameters. Sorts can be on the computed metrics or on the bucket values.
* `bucketSizeLimit`: The number of buckets to include. This value is applied to each dimension.
* `metrics`: List of metrics to compute for the buckets. Currently supported metrics are `sum(col)`, `avg(col)`, `min(col)`, `max(col)`, `count(*)`.

[[StreamingExpressions-Syntax.2]]
==== Syntax

Example 1:

[source,java]
----
facet(collection1, 
      q="*:*", 
      buckets="a_s",
      bucketSorts="sum(a_i) desc",
      bucketSizeLimit=100,
      sum(a_i), 
      sum(a_f), 
      min(a_i), 
      min(a_f), 
      max(a_i), 
      max(a_f),
      avg(a_i), 
      avg(a_f), 
      count(*))
----

The example above shows a facet function with rollups over a single bucket, where the buckets are returned in descending order by the calculated value of the `sum(a_i)` metric.

Example 2:

[source,java]
----
facet(collection1, 
      q="*:*", 
      buckets="year_i, month_i, day_i",
      bucketSorts="year_i desc, month_i desc, day_i desc",
      bucketSizeLimit=100,
      sum(a_i), 
      sum(a_f), 
      min(a_i), 
      min(a_f), 
      max(a_i), 
      max(a_f),
      avg(a_i), 
      avg(a_f), 
      count(*))
----

The example above shows a facet function with rollups over three buckets, where the buckets are returned in descending order by bucket value.

[[StreamingExpressions-gatherNodes]]
=== gatherNodes

The `gatherNodes` function provides breadth-first graph traversal. For details, see the section <<graph-traversal.adoc#,Graph Traversal>>.

[[StreamingExpressions-random]]
=== random

The `random` function searches a SolrCloud collection and emits a pseudo-random set of results that match the query. Each invocation of random will return a different pseudo-random result set.

[[StreamingExpressions-Parameters.3]]
==== Parameters

* `collection`: (Mandatory) Collection the stats will be aggregated from.
* `q`: (Mandatory) The query to build the aggregations from.
* `rows`: (Mandatory) The number of pseudo-random results to return.
* fl: (Mandatory) The field list to return.
* `fq`: (Optional) Filter query

[[StreamingExpressions-Syntax.3]]
==== Syntax

[source,java]
----
random(baskets, 
       q="productID:productX", 
       rows="100", 
       fl="basketID") 
----

In the example above the `random` function is searching the baskets collections for all rows where "productID:productX". It will return 100 pseudo-random results. The field list returned is the basketID.

[[StreamingExpressions-shortestPath]]
=== shortestPath

The `shortestPath` function is an implementation of a shortest path graph traversal. The `shortestPath` function performs an iterative breadth-first search through an unweighted graph to find the shortest paths between two nodes in a graph. The `shortestPath` function emits a tuple for each path found. Each tuple emitted will contain a `path` key which points to a `List` of nodeIDs comprising the path.

[[StreamingExpressions-Parameters.4]]
==== Parameters

* `collection`: (Mandatory) The collection that the topic query will be run on.
* `from`: (Mandatory) The nodeID to start the search from
* `to`: (Mandatory) The nodeID to end the search at
* `edge`: (Mandatory) Syntax: `from_field=to_field`. The `from_field` defines which field to search from. The `to_field` defines which field to search to. See example below for a detailed explanation.
* `threads`: (Optional : Default 6) The number of threads used to perform the partitioned join in the traversal.
* `partitionSize`: (Optional : Default 250) The number of nodes in each partition of the join.
* `fq`: (Optional) Filter query
* `maxDepth`: (Mandatory) Limits to the search to a maximum depth in the graph.

*Syntax*

[source,java]
----
shortestPath(collection, 
             from="john@company.com", 
             to="jane@company.com",
             edge="from_address=to_address",
             threads="6",
             partitionSize="300", 
             fq="limiting query", 
             maxDepth="4")
----

The expression above performs a breadth-first search to find the shortest paths in an unweighted, directed graph.

The search starts from the nodeID "john@company.com" in the `from_address` field and searches for the nodeID "jane@company.com" in the `to_address` field. This search is performed iteratively until the `maxDepth` has been reached. Each level in the traversal is implemented as a parallel partitioned nested loop join across the entire collection. The `threads` parameter controls the number of threads performing the join at each level, while the `partitionSize` parameter controls the of number of nodes in each join partition. The `maxDepth` parameter controls the number of levels to traverse. `fq` is a limiting query applied to each level in the traversal.

[[StreamingExpressions-stats]]
=== stats

The `stats` function gathers simple aggregations for a search result set. The stats function does not support rollups over buckets, so the stats stream always returns a single tuple with the rolled up stats. Under the covers the stats function pushes down the generation of the stats into the search engine using the StatsComponent. The stats function currently supports the following metrics: `count(*)`, `sum()`, `avg()`, `min()`, and `max()`.

[[StreamingExpressions-Parameters.5]]
==== Parameters

* `collection`: (Mandatory) Collection the stats will be aggregated from.
* `q`: (Mandatory) The query to build the aggregations from.
* `metrics`: (Mandatory) The metrics to include in the result tuple. Current supported metrics are `sum(col)`, `avg(col)`, `min(col)`, `max(col)` and `count(*)`

[[StreamingExpressions-Syntax.4]]
==== Syntax

[source,java]
----
stats(collection1, 
      q=*:*, 
      sum(a_i), 
      sum(a_f), 
      min(a_i), 
      min(a_f), 
      max(a_i), 
      max(a_f), 
      avg(a_i), 
      avg(a_f), 
      count(*))
----

[[StreamingExpressions-topic]]
=== topic

The `topic` function provides publish/subscribe messaging capabilities built on top of SolrCloud. The topic function allows users to subscribe to a query. The function then provides one-time delivery of new or updated documents that match the topic query. The initial call to the topic function establishes the checkpoints for the specific topic ID. Subsequent calls to the same topic ID will return new or updated documents that match the topic query.

[WARNING]
====

The topic function should be considered in beta until https://issues.apache.org/jira/browse/SOLR-8709[SOLR-8709] is committed and released.

====

[[StreamingExpressions-Parameters.6]]
==== Parameters

* `checkpointCollection`: (Mandatory) The collection where the topic checkpoints are stored.
* `collection`: (Mandatory) The collection that the topic query will be run on.
* `id`: (Mandatory) The unique ID for the topic. The checkpoints will be saved under this id.
* `q`: (Mandatory) The topic query.
* `fl`: (Mandatory) The field list returned by the topic function.

[[StreamingExpressions-Syntax.5]]
==== Syntax

[source,java]
----
topic(checkpointCollection,
      collection,
      id="uniqueId", 
      q="topic query",
      fl="id, name, country") 
----

[[StreamingExpressions-StreamDecorators]]
== Stream Decorators

Stream decorators wrap other stream functions or perform operations on the stream. The are currently many stream decorators available: **complement**, **daemon**, **innerJoin**, **intersect**, **hashJoin**, **merge**, **leftOuterJoin**, **outerHashJoin**, **parallel**, **reduce**, **rollup**, **select**, **top**, **unique**, and *update.*

[[StreamingExpressions-complement]]
=== complement

The `complement` function wraps two streams (A and B) and emits tuples from A which do not exist in B. The tuples are emitted in the order in which they appear in stream A. Both streams must be sorted by the fields being used to determine equality (using the `on` parameter).

[[StreamingExpressions-Parameters.7]]
==== Parameters

* `StreamExpression for StreamA`
* `StreamExpression for StreamB`
* `on`: Fields to be used for checking equality of tuples between A and B. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.6]]
==== Syntax

[source,java]
----
complement(
  search(collection1, q=a_s:(setA || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  search(collection1, q=a_s:(setB || setAB), fl="id,a_s,a_i", sort="a_i asc"),
  on="a_i"
)
 
complement(
  search(collection1, q=a_s:(setA || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  search(collection1, q=a_s:(setB || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  on="a_i,a_s"
)
----

[[StreamingExpressions-daemon]]
=== daemon

The `daemon` function wraps another function and runs it at intervals using an internal thread. The daemon function can be used to provide both continuous push and pull streaming.

[[StreamingExpressions-Continuouspushstreaming]]
==== *Continuous push streaming*

With continuous push streaming the daemon function wraps another function and is then sent to the `/stream` handler for execution. The `/stream` handler recognizes the daemon function and keeps it resident in memory, so it can run it's internal function at intervals.

In order to facilitate the pushing of tuples, the daemon function must wrap another stream decorator that pushes the tuples somewhere. One example of this is the `update` function, which wraps a stream and sends the tuples to another SolrCloud collection for indexing.

*Example:*

[source,java]
----
daemon(id="uniqueId", 
       runInterval="1000",
       update(destinationCollection, 
              batchSize=100, 
              topic(checkpointCollection, 
                    topicCollection, 
                    q="topic query", 
                    fl="id, title, abstract, text", 
                    id="topicId")
               )
        )
----

The sample code above shows a `daemon` function wrapping an `update `function, which is wrapping a `topic` function. When this expression is sent to the `/stream` handler, the `/stream` hander sees the daemon function and keeps it in memory where it will run at intervals. In this particular example, the daemon function will run the `update` function every second. The `update` function is wrapping a `topic` function, which returns all new documents for a specific query. The update function will send the new documents to another collection to be indexed.

The effect of this is to continuously push new documents that match a specific query into another collection. Custom push functions can be plugged in that push documents out of Solr and into other systems, such as Kafka or an email system.

Push streaming can also be used for continuous background aggregation scenarios where aggregates are rolled up in the background at intervals and pushed to other Solr collections. Another use case is continuous background machine learning model optimization, where the optimized model is pushed to another Solr collection where they can be integrated into queries.

The `/stream` handler supports a small set commands for listing and controlling daemon functions:

[source,java]
----
http://localhost:8983/collection/stream?action=list
----

This command will provide a listing of the current daemon's running on the specific node along with there current state.

[source,java]
----
http://localhost:8983/collection/stream?action=stop&id=daemonId
----

This command will stop a specific daemon function but leave it resident in memory

[source,java]
----
http://localhost:8983/collection/stream?action=start&id=daemonId
----

This command will start a specific daemon function that has been stopped.

[source,java]
----
http://localhost:8983/collection/stream?action=kill&id=daemonId
----

This command will stop a specific daemon function and remove it from memory.

[[StreamingExpressions-ContinousPullStreaming]]
==== *Continous Pull Streaming*

The DaemonStream java class (part of the Solrj libraries) can also be embedded in a java application to provide continuous pull streaming. Sample code:

[source,java]
----
StreamContext context = new StreamContext()
SolrClientCache cache = new SolrClientCache();
context.setSolrClientCache(cache);

Map topicQueryParams = new HashMap();  
topicQueryParams.put("q","hello");  // The query for the topic
topicQueryparams.put("rows", "500"); // How many rows to fetch during each run
topicQueryparams.put("fl", "id, "title"); // The field list to return with the documents

TopicStream topicStream = new TopicStream(zkHost,        // Host address for the zookeeper service housing the collections 
                                         "checkpoints",  // The collection to store the topic checkpoints
                                         "topicData",    // The collection to query for the topic records
                                         "topicId",      // The id of the topic
                                         -1,             // checkpoint every X tuples, if set -1 it will checkpoint after each run.
                                          topicQueryParams); // The query parameters for the TopicStream

DaemonStream daemonStream = new DaemonStream(topicStream,             // The underlying stream to run. 
                                             "daemonId",              // The id of the daemon
                                             1000,                    // The interval at which to run the internal stream
                                             500);                    // The internal queue size for the daemon stream. Tuples will be placed in the queue
                                                                      // as they are read by the internal internal thread.
                                                                      // Calling read() on the daemon stream reads records from the internal queue.
                                                                       
daemonStream.setStreamContext(context);

daemonStream.open();
 
//Read until it's time to shutdown the DaemonStream. You can define the shutdown criteria.
while(!shutdown()) {
    Tuple tuple = daemonStream.read() // This will block until tuples become available from the underlying stream (TopicStream)
                                      // The EOF tuple (signaling the end of the stream) will never occur until the DaemonStream has been shutdown.
    //Do something with the tuples
}
 
// Shutdown the DaemonStream.
daemonStream.shutdown();
 
//Read the DaemonStream until the EOF Tuple is found.
//This allows the underlying stream to perform an orderly shutdown.
 
while(true) {
    Tuple tuple = daemonStream.read();
    if(tuple.EOF) {
        break;
    } else {
        //Do something with the tuples.
    }
}
//Finally close the stream
daemonStream.close();
----

[[StreamingExpressions-leftOuterJoin]]
=== leftOuterJoin

The `leftOuterJoin` function wraps two streams, Left and Right, and emits tuples from Left. If there is a tuple in Right equal (as defined by `on`) then the values in that tuple will be included in the emitted tuple. An equal tuple in Right *need not* exist for the Left tuple to be emitted. This supports one-to-one, one-to-many, many-to-one, and many-to-many left outer join scenarios. The tuples are emitted in the order in which they appear in the Left stream. Both streams must be sorted by the fields being used to determine equality (using the `on` parameter). If both tuples contain a field of the same name then the value from the Right stream will be used in the emitted tuple.

You can wrap the incoming streams with a `select` function to be specific about which field values are included in the emitted tuple.

[[StreamingExpressions-Parameters.8]]
==== Parameters

* `StreamExpression for StreamLeft`
* `StreamExpression for StreamRight`
* `on`: Fields to be used for checking equality of tuples between Left and Right. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.7]]
==== Syntax

[source,java]
----
leftOuterJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  search(pets, q=type:cat, fl="personId,petName", sort="personId asc"),
  on="personId"
)

leftOuterJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  search(pets, q=type:cat, fl="ownerId,petName", sort="ownerId asc"),
  on="personId=ownerId"
)
 
leftOuterJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  select(
    search(pets, q=type:cat, fl="ownerId,name", sort="ownerId asc"),
    ownerId,
    name as petName
  ),
  on="personId=ownerId"
)
----

[[StreamingExpressions-hashJoin]]
=== hashJoin

The `hashJoin` function wraps two streams, Left and Right, and for every tuple in Left which exists in Right will emit a tuple containing the fields of both tuples. This supports one-to-one, one-to-many, many-to-one, and many-to-many inner join scenarios. The tuples are emitted in the order in which they appear in the Left stream. The order of the streams does not matter. If both tuples contain a field of the same name then the value from the Right stream will be used in the emitted tuple.

You can wrap the incoming streams with a `select` function to be specific about which field values are included in the emitted tuple.

The hashJoin function can be used when the tuples of Left and Right cannot be put in the same order. Because the tuples are out of order this stream functions by reading all values from the Right stream during the open operation and will store all tuples in memory. The result of this is a memory footprint equal to the size of the Right stream.

[[StreamingExpressions-Parameters.9]]
==== Parameters

* `StreamExpression for StreamLeft`
* `hashed=StreamExpression for StreamRight`
* `on`: Fields to be used for checking equality of tuples between Left and Right. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.8]]
==== Syntax

[source,java]
----
hashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=search(pets, q=type:cat, fl="personId,petName", sort="personId asc"),
  on="personId"
)

hashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=search(pets, q=type:cat, fl="ownerId,petName", sort="ownerId asc"),
  on="personId=ownerId"
)
 
hashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=select(
    search(pets, q=type:cat, fl="ownerId,name", sort="ownerId asc"),
    ownerId,
    name as petName
  ),
  on="personId=ownerId"
)
----

[[StreamingExpressions-innerJoin]]
=== innerJoin

Wraps two streams Left and Right and for every tuple in Left which exists in Right will emit a tuple containing the fields of both tuples. This supports one-one, one-many, many-one, and many-many inner join scenarios. The tuples are emitted in the order in which they appear in the Left stream. Both streams must be sorted by the fields being used to determine equality (the 'on' parameter). If both tuples contain a field of the same name then the value from the Right stream will be used in the emitted tuple. You can wrap the incoming streams with a select(...) to be specific about which field values are included in the emitted tuple.

[[StreamingExpressions-Parameters.10]]
==== Parameters

* `StreamExpression for StreamLeft`
* `StreamExpression for StreamRight`
* `on`: Fields to be used for checking equality of tuples between Left and Right. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.9]]
==== Syntax

[source,java]
----
innerJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  search(pets, q=type:cat, fl="personId,petName", sort="personId asc"),
  on="personId"
)

innerJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  search(pets, q=type:cat, fl="ownerId,petName", sort="ownerId asc"),
  on="personId=ownerId"
)
 
innerJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  select(
    search(pets, q=type:cat, fl="ownerId,name", sort="ownerId asc"),
    ownerId,
    name as petName
  ),
  on="personId=ownerId"
)
----

[[StreamingExpressions-intersect]]
=== intersect

The `intersect` function wraps two streams, A and B, and emits tuples from A which *DO* exist in B. The tuples are emitted in the order in which they appear in stream A. Both streams must be sorted by the fields being used to determine equality (the `on` parameter). Only tuples from A are emitted.

[[StreamingExpressions-Parameters.11]]
==== Parameters

* `StreamExpression for StreamA`
* `StreamExpression for StreamB`
* `on`: Fields to be used for checking equality of tuples between A and B. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.10]]
==== Syntax

[source,java]
----
intersect(
  search(collection1, q=a_s:(setA || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  search(collection1, q=a_s:(setB || setAB), fl="id,a_s,a_i", sort="a_i asc"),
  on="a_i"
)
 
intersect(
  search(collection1, q=a_s:(setA || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  search(collection1, q=a_s:(setB || setAB), fl="id,a_s,a_i", sort="a_i asc, a_s asc"),
  on="a_i,a_s"
)
----

[[StreamingExpressions-merge]]
=== merge

The `merge` function merges two or more streaming expressions and maintains the ordering of the underlying streams. Because the order is maintained, the sorts of the underlying streams must line up with the on parameter provided to the merge function.

[[StreamingExpressions-Parameters.12]]
==== Parameters

* `StreamExpression A`
* `StreamExpression B`
* `Optional StreamExpression C,D,....Z`
* `on`: Sort criteria for performing the merge. Of the form `fieldName order` where order is `asc` or `desc`. Multiple fields can be provided in the form `fieldA order, fieldB order`.

[[StreamingExpressions-Syntax.11]]
==== Syntax

[source,java]
----
# Merging two stream expressions together
merge(
      search(collection1, 
             q="id:(0 3 4)", 
             fl="id,a_s,a_i,a_f", 
             sort="a_f asc"),
      search(collection1, 
             q="id:(1)", 
             fl="id,a_s,a_i,a_f", 
             sort="a_f asc"),
      on="a_f asc") 
----

[source,py]
----
# Merging four stream expressions together. Notice that while the sorts of each stream are not identical they are 
# comparable. That is to say the first N fields in each stream's sort matches the N fields in the merge's on clause.
merge(
      search(collection1, 
             q="id:(0 3 4)", 
             fl="id,fieldA,fieldB,fieldC", 
             sort="fieldA asc, fieldB desc"),
      search(collection1, 
             q="id:(1)", 
             fl="id,fieldA", 
             sort="fieldA asc"),
      search(collection2, 
             q="id:(10 11 13)", 
             fl="id,fieldA,fieldC", 
             sort="fieldA asc"),
      search(collection3, 
             q="id:(987)", 
             fl="id,fieldA,fieldC", 
             sort="fieldA asc"),
      on="fieldA asc") 
----

[[StreamingExpressions-outerHashJoin]]
=== outerHashJoin

The `outerHashJoin` function wraps two streams, Left and Right, and emits tuples from Left. If there is a tuple in Right equal (as defined by the `on` parameter) then the values in that tuple will be included in the emitted tuple. An equal tuple in Right *need not* exist for the Left tuple to be emitted. This supports one-to-one, one-to-many, many-to-one, and many-to-many left outer join scenarios. The tuples are emitted in the order in which they appear in the Left stream. The order of the streams does not matter. If both tuples contain a field of the same name then the value from the Right stream will be used in the emitted tuple.

You can wrap the incoming streams with a `select` function to be specific about which field values are included in the emitted tuple.

The outerHashJoin stream can be used when the tuples of Left and Right cannot be put in the same order. Because the tuples are out of order, this stream functions by reading all values from the Right stream during the open operation and will store all tuples in memory. The result of this is a memory footprint equal to the size of the Right stream.

[[StreamingExpressions-Parameters.13]]
==== Parameters

* `StreamExpression for StreamLeft`
* `hashed=StreamExpression for StreamRight`
* `on`: Fields to be used for checking equality of tuples between Left and Right. Can be of the format `on="fieldName"`, `on="fieldNameInLeft=fieldNameInRight"`, or `on="fieldName, otherFieldName=rightOtherFieldName"`.

[[StreamingExpressions-Syntax.12]]
==== Syntax

[source,java]
----
outerHashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=search(pets, q=type:cat, fl="personId,petName", sort="personId asc"),
  on="personId"
)

outerHashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=search(pets, q=type:cat, fl="ownerId,petName", sort="ownerId asc"),
  on="personId=ownerId"
)
 
outerHashJoin(
  search(people, q=*:*, fl="personId,name", sort="personId asc"),
  hashed=select(
    search(pets, q=type:cat, fl="ownerId,name", sort="ownerId asc"),
    ownerId,
    name as petName
  ),
  on="personId=ownerId"
)
----

[[StreamingExpressions-parallel]]
=== parallel

The `parallel` function wraps a streaming expression and sends it to N worker nodes to be processed in parallel.

The parallel function requires that the `partitionKeys` parameter be provided to the underlying searches. The `partitionKeys` parameter will partition the search results (tuples) across the worker nodes. Tuples with the same values in the partitionKeys field will be shuffled to the same worker nodes.

The parallel function maintains the sort order of the tuples returned by the worker nodes, so the sort criteria of the parallel function must match up with the sort order of the tuples returned by the workers.

.Worker Collections
[TIP]
====

The worker nodes can be from the same collection as the data, or they can be a different collection entirely, even one that only exists for parallel streaming expressions. A worker collection can be any SolrCloud collection that has the `/stream` handler configured. Unlike normal SolrCloud collections, worker collections don't have to hold any data. Worker collections can be empty collections that exist only to execute streaming expressions.

====

[[StreamingExpressions-Parameters.14]]
==== Parameters

* `collection`: Name of the worker collection to send the StreamExpression to.
* `StreamExpression`: Expression to send to the worker collection.
* `workers`: Number of workers in the worker collection to send the expression to.
* `zkHost`: (Optional) The ZooKeeper connect string where the worker collection resides.
* `sort`: The sort criteria for ordering tuples returned by the worker nodes.

[[StreamingExpressions-Syntax.13]]
==== Syntax

[source,java]
----
 parallel(workerCollection, 
          reduce(
                 search(collection1, q=*:*, fl="id,a_s,a_i,a_f", sort="a_s desc", partitionKeys="a_s"),
                 by="a_s",
                 group(sort="a_f desc", n="4"))
          workers="20", 
          zkHost="localhost:9983", 
          sort="a_s desc")
----

The expression above shows a parallel function wrapping a reduce function. This will cause the reduce function to be run in parallel across 20 worker nodes.

[[StreamingExpressions-reduce]]
=== reduce

The `reduce` function wraps an internal stream and groups tuples by common fields.

Each tuple group is operated on as a single block by a pluggable reduce operation. The group operation provided with Solr implements distributed grouping functionality. The group operation also serves as an example reduce operation that can be referred to when building custom reduce operations.

[IMPORTANT]
====

The reduce function relies on the sort order of the underlying stream. Accordingly the sort order of the underlying stream must be aligned with the group by field.

====

[[StreamingExpressions-Parameters.15]]
==== Parameters

* `StreamExpression`: (Mandatory)
* `by`: (Mandatory) A comma separated list of fields to group by.
* `Reduce Operation`: (Mandatory)

[[StreamingExpressions-Syntax.14]]
==== Syntax

[source,java]
----
reduce(
   search(collection1, q=*:*, fl="id,a_s,a_i,a_f", sort="a_s asc, a_f asc"),
   by="a_s",
   group(sort="a_f desc", n="4")
)
----

[[StreamingExpressions-rollup]]
=== rollup

The `rollup` function wraps another stream function and rolls up aggregates over bucket fields. The rollup function relies on the sort order of the underlying stream to rollup aggregates one grouping at a time. Accordingly, the sort order of the underlying stream must match the fields in the `over` parameter of the rollup function.

The rollup function also needs to process entire result sets in order to perform it's aggregations. When the underlying stream is the `search` function, the `/export` handler can be used to provide full sorted result sets to the rollup function. This sorted approach allows the rollup function to perform aggregations over very high cardinality fields. The disadvantage of this approach is that the tuples must be sorted and streamed across the network to a worker node to be aggregated. For faster aggregation over low to moderate cardinality fields, the `facet` function can be used.

[[StreamingExpressions-Parameters.16]]
==== Parameters

* `StreamExpression` (Mandatory)
* `over`: (Mandatory) A list of fields to group by.
* `metrics`: (Mandatory) The list of metrics to compute. Currently supported metrics are `sum(col)`, `avg(col)`, `min(col)`, `max(col)`, `count(*)`.

[[StreamingExpressions-Syntax.15]]
==== Syntax

[source,java]
----
rollup(
   search(
      collection1, q=*:*, fl="a_s,a_i,a_f", qt="/export", sort="a_s asc"),
   over="a_s",
   sum(a_i),
   sum(a_f),
   min(a_i),
   min(a_f),
   max(a_i),
   max(a_f),
   avg(a_i),
   avg(a_f),
   count(*)
)
----

The example about shows the rollup function wrapping the search function. Notice that search function is using the `/export` handler to provide the entire result set to the rollup stream. Also notice that the search function's *sort param* matches up with the rollup's `over` parameter. This allows the rollup function to rollup the over the `a_s` field, one group at a time.

[[StreamingExpressions-select]]
=== select

The `select` function wraps a streaming expression and outputs tuples containing a subset or modified set of fields from the incoming tuples. The list of fields included in the output tuple can contain aliases to effectively rename fields. One can provide a list of operations to perform on any fields, such as `replace` to replace the value of a field with some other value or the value of another field in the tuple.

[[StreamingExpressions-Parameters.17]]
==== Parameters

* `StreamExpression`
* `fieldName`: name of field to include in the output tuple (can include multiple of these) outputTuple[fieldName] = inputTuple[fieldName]
* `fieldName as aliasFieldName`: aliased field name to include in the output tuple (can include multiple of these) outputTuple[aliasFieldName] = incomingTuple[fieldName]
* `replace(fieldName, value, withValue=replacementValue)`: if incomingTuple[fieldName] == value then outgoingTuple[fieldName] will be set to replacementValue. value can be the string "null" to replace a null value with some other value
* `replace(fieldName, value, withField=otherFieldName)`: if incomingTuple[fieldName] == value then outgoingTuple[fieldName] will be set to the value of incomingTuple[otherFieldName]. value can be the string "null" to replace a null value with some other value

[[StreamingExpressions-Syntax.16]]
==== Syntax

[source,java]
----
// output tuples with fields teamName, wins, and losses where a null value for wins or losses is translated to the value of 0
select(
  search(collection1, fl="id,teamName_s,wins,losses", q="*:*", sort="id asc"),
  teamName_s as teamName,
  wins,
  losses,
  replace(wins,null,withValue=0),
  replace(losses,null,withValue=0)
)
----

[[StreamingExpressions-sort]]
=== sort

The `sort` function wraps a streaming expression and re-orders the tuples. The sort function emits all incoming tuples in the new sort order. The sort function reads all tuples from the incoming stream, re-orders them using an algorithm with `O(nlog(n))` performance characteristics, where n is the total number of tuples in the incoming stream, and then outputs the tuples in the new sort order. Because all tuples are read into memory, the memory consumption of this function grows linearly with the number of tuples in the incoming stream.

[[StreamingExpressions-Parameters.18]]
==== Parameters

* `StreamExpression`
* `by`: Sort criteria for re-ordering the tuples

[[StreamingExpressions-Syntax.17]]
==== Syntax

The expression below finds dog owners and orders the results by owner and pet name. Notice that it uses an efficient innerJoin by first ordering by the person/owner id and then re-orders the final output by the owner and pet names.

[source,java]
----
sort(
  innerJoin(
    search(people, q=*:*, fl="id,name", sort="id asc"),
    search(pets, q=type:dog, fl="owner,petName", sort="owner asc"), 
    on="id=owner"
  ),
  by="name asc, petName asc"
)
----

[[StreamingExpressions-top]]
=== top

The `top` function wraps a streaming expression and re-orders the tuples. The top function emits only the top N tuples in the new sort order. The top function re-orders the underlying stream so the sort criteria *does not* have to match up with the underlying stream.

[[StreamingExpressions-Parameters.19]]
==== Parameters

* `n`: Number of top tuples to return.
* `StreamExpression`
* `sort`: Sort criteria for selecting the top N tuples.

[[StreamingExpressions-Syntax.18]]
==== Syntax

The expression below finds the top 3 results of the underlying search. Notice that it reverses the sort order. The top function re-orders the results of the underlying stream.

[source,java]
----
top(n=3,
     search(collection1, 
            q="*:*",
            qt="/export", 
            fl="id,a_s,a_i,a_f", 
            sort="a_f desc, a_i desc"),
      sort="a_f asc, a_i asc")
----

[[StreamingExpressions-unique]]
=== unique

The `unique` function wraps a streaming expression and emits a unique stream of tuples based on the `over` parameter. The unique function relies on the sort order of the underlying stream. The `over` parameter must match up with the sort order of the underlying stream.

The unique function implements a non-co-located unique algorithm. This means that records with the same unique `over` field do not need to be co-located on the same shard. When executed in the parallel, the `partitionKeys` parameter must be the same as the unique `over` field so that records with the same keys will be shuffled to the same worker.

[[StreamingExpressions-Parameters.20]]
==== Parameters

* `StreamExpression`
* `over`: The unique criteria.

[[StreamingExpressions-Syntax.19]]
==== Syntax

[source,java]
----
unique(
  search(collection1,
         q="*:*",
         qt="/export",
         fl="id,a_s,a_i,a_f",
         sort="a_f asc, a_i asc"),
  over="a_f")
----

[[StreamingExpressions-update]]
=== update

The `update` function wraps another functions and sends the tuples to a SolrCloud collection for indexing.

[[StreamingExpressions-Parameters.21]]
==== Parameters

* `destinationCollection`: (Mandatory) The collection where the tuples will indexed.
* `batchSize`: (Mandatory) The indexing batch size.
* `StreamExpression`: (Mandatory)

[[StreamingExpressions-Syntax.20]]
==== Syntax

[source,java]
----
 update(destinationCollection, 
        batchSize=500, 
        search(collection1, 
               q=*:*, 
               fl="id,a_s,a_i,a_f,s_multi,i_multi", 
               sort="a_f asc, a_i asc"))
 
----

The example above sends the tuples returned by the `search` function to the `destinationCollection` to be indexed.
